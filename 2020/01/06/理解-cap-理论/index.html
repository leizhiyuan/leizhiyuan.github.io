<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>理解 CAP 理论 - bystander's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="bystander"><meta name=description content="背景 CAP理论 实际上听起来非常简单，但是有时候，遇到一些具体的问题的时候， 还是不能很清晰的分辨出来，到底是CP还是AP，以及一些其他的问题。因此，专门作为&amp;quot;生产者&amp;quot;来学习下，加深理解。
首先，在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer&amp;rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
1.一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 2.可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 3.分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）
根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。
这个定理起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在2000年的分布式计算原理研讨会（PODC）上提出的一个猜想。 在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。
举例 假设你明天就要放长假了，你想买一本战争与和平的书籍，你最喜欢的在线商城里面只有一本了。
一致性： Consistency，在Gilbert and Lynch 的论文里，他们也用 “Atomic” 原子性来代替一致性这个单词。 在买书的这个例子里，你要么就是把书放到了购物车，要么就是放失败了，要么付款，要么没付款，不可能说放了一半，或者说买了一半。只有一本书，如果两个客户都准备买，缺乏一致性的话，如果两个人都完成了下单，可能会出问题。比如两个人都下了单，当然，在这个例子中，并不严重。
我们也可以用数据库来解决这个问题，数据库里有个字段减去个1，然后当及其他客户也要付款的时候，我们提示他没了。
数据库看很好用。因为具有ACID的能力。既有一致性，又有原子性，中间状态对第二个客户端是不可见的。是隔离的。因为第二个客户端再下单的时候，另一个用户在事务中的话，就锁住了数据库那条记录。
可用性： 可用性就是说，当你需要的时候，大部分情况下，服务都是可以为你服务的。以买书为例，在用户A开启事务的时候，有那么几毫秒是锁表的，这个阶段，服务可以认为对其他用户是不可用的。并不是说要时时刻刻可用，一般会有个可用率的指标。如果记不住，可以通过这里来计算： https://uptime.is/99.99999
分区容错： 如果你就一个数据库，一个服务端，那一般也都是原子的，如果挂了，服务不可用，但是数据还是一致的。
一旦你把数据和代码逻辑，开始部署在不同的节点上，这时候就存在分区。如Node A 不能和Node B通信来，这种分区问题经常出现。
用图来证明：
在一个网络环境下，有两个节点，N1和N2，共享相同的数据V，在买书这个例子中，这个数据里面存储的就是有多少本书，假设初始值是V0，在N1上运行一个买卖算法，A，假设这个算法没有bug，非常正确，可心来，N2也是类似的，叫做B，A写了一个新值到V中，然后B从V中读取。
正常流程是这样，A写完之后，N1和N2通过一个消息（非具体的消息），将这个值同步给N2。然后B也就能读到了。
1.A写了一个值V1 2.从N1发了个消息M到N2。 3.B也能从V中读到V1了
但是，现实没有这么美好
网络发生了分区，从N1到N2的消息没有投递成功。这样，到第三步的时候，N2读到了V0这个错误的值。
如果M是一个异步消息，那么N1都没办法知道N2是不是收到了。即使有办法保证M这个消息一定发出去了。那么N1也没办法知道，这个消息是不是被投递了，也不知道N2处理的时候，有没有问题。那么，如果我们把M改成同步消息呢。也不行，因为这意味着将A写值到N1，和从N1到N2更新事件是一个原子操作。
CAP告诉我们，如果我们想要A和B高度可用（低延迟），我们就要N1和N2保持分区容错，比如出现消息丢失，消息未投递，硬件故障，或者处理失败。这种情况下，就会出现有时候一些节点任务V是V0，另一些节点认为是V1.
如果有一个事务，叫做a1，a1可能是一个写操作，a2是一个读操作，在本地系统中，通过数据库或者自己加锁，加隔离是很简单的。可以强制a1写完之后，a2才发生，但是在分布式环境中，一旦加了这些东西，就影响额分区容错和可用性。
处理CAP  CA 不要 P，不要分区容错  不保证分区容错，那么你可以部署在一个台机器上，但是容量受限。并且还是会存在网络问题。分布式环境下，网络分区是必然的。除非你就不想做分布式。
在分布式的环境下，网络无法做到100%可靠，有可能出现故障，因此分区是一个必须的选项，如果选择了CA而放弃了P，若发生分区现象，为了保证C，系统需要禁止写入，此时就与A发生冲突，如果是为了保证A，则会出现正常的分区可以写入数据，有故障的分区不能写入数据，则与C就冲突了。因此分布式系统理论上不可能选择CA架构，而必须选择CP或AP架构。
从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。
所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。
CP 不要 A 不要可用性  当你想要分区容错的时候，并且可以容忍长时间的停机或者无影响。就可以舍弃可用性。
一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。
设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，
常用的Zookeeper也是在CAP三者之中选择优先保证CP的。ZooKeeper是个CP 的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分区具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。
ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成CP而不是AP特性的了。从实际情况来分析，在使用 Zookeeper 获取服务列表时，如果 ZooKeeper 正在选举或者 ZooKeeper 集群中半数以上的机器不可用，那么将无法获取数据。所以说，ZooKeeper 不能保证服务可用性。
Eureka 则是一个AP系统，一部分节点挂掉不会影响到正常节点的工作，不会出现类似 ZK 的选举 Leader 的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点。 只要有一台 Eureka 存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。
SofaRegistry 也是一个AP系统。
AP 不要 C 不要一致性  要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。
这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。
举个例子，你在12306买票的时候肯定遇到过这种场景，你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。
但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。也就是说，最终不会出现，2个人买到了同样的票。
对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。
怎么选择呢 既要又要。那怎么办？
虽然三个不能保证，但我们能不能在一致性上作出一些妥协，不追求时时刻刻的强一致性，转而追求最终一致性，所以引入 BASE 理论。 在分布式事务中，BASE 最重要是为 CAP 提出了最终一致性的解决方案，BASE 强调牺牲高一致性，从而获取可用性，数据允许在一段时间内不一致，只要保证最终一致性就可以了，实现最终一致性。
弱一致性：系统不能保证后续访问返回更新的值。需要在一些条件满足之后，更新的值才能返回。从更新操作开始，到系统保证任何观察者总是看到更新的值的这期间被称为不一致窗口。
最终一致性：这是弱一致性的特殊形式;存储系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。
BASE 模型 BASE 模型是传统 ACID 模型的反面，不同于 ACID，BASE 强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。 BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性：Basically Available 基本可用。 支持分区失败(e.g. sharding碎片划分数据库)Soft state 软状态，状态可以有一段时间不同步，异步。 Eventually consistent 最终一致，最终数据是一致的就可以了，而不是时时一致。
参考 http://www.julianbrowne.com/article/brewers-cap-theorem
https://www.cnblogs.com/13yan/p/9243669.html"><meta name=generator content="Hugo 0.79.1"><link rel=canonical href=http://blog.leaver.me/2020/01/06/%E7%90%86%E8%A7%A3-cap-%E7%90%86%E8%AE%BA/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.9912dff54a8e025fc24be2961418c8ffa2bfa95092ee8ce19d13e9da0b3d6048.css integrity="sha256-mRLf9UqOAl/CS+KWFBjI/6K/qVCS7ozhnRPp2gs9YEg=" media=screen crossorigin=anonymous><meta property="og:title" content="理解 CAP 理论"><meta property="og:description" content="背景 CAP理论 实际上听起来非常简单，但是有时候，遇到一些具体的问题的时候， 还是不能很清晰的分辨出来，到底是CP还是AP，以及一些其他的问题。因此，专门作为&#34;生产者&#34;来学习下，加深理解。
首先，在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer&rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
1.一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 2.可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 3.分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）
根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。
这个定理起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在2000年的分布式计算原理研讨会（PODC）上提出的一个猜想。 在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。
举例 假设你明天就要放长假了，你想买一本战争与和平的书籍，你最喜欢的在线商城里面只有一本了。
一致性： Consistency，在Gilbert and Lynch 的论文里，他们也用 “Atomic” 原子性来代替一致性这个单词。 在买书的这个例子里，你要么就是把书放到了购物车，要么就是放失败了，要么付款，要么没付款，不可能说放了一半，或者说买了一半。只有一本书，如果两个客户都准备买，缺乏一致性的话，如果两个人都完成了下单，可能会出问题。比如两个人都下了单，当然，在这个例子中，并不严重。
我们也可以用数据库来解决这个问题，数据库里有个字段减去个1，然后当及其他客户也要付款的时候，我们提示他没了。
数据库看很好用。因为具有ACID的能力。既有一致性，又有原子性，中间状态对第二个客户端是不可见的。是隔离的。因为第二个客户端再下单的时候，另一个用户在事务中的话，就锁住了数据库那条记录。
可用性： 可用性就是说，当你需要的时候，大部分情况下，服务都是可以为你服务的。以买书为例，在用户A开启事务的时候，有那么几毫秒是锁表的，这个阶段，服务可以认为对其他用户是不可用的。并不是说要时时刻刻可用，一般会有个可用率的指标。如果记不住，可以通过这里来计算： https://uptime.is/99.99999
分区容错： 如果你就一个数据库，一个服务端，那一般也都是原子的，如果挂了，服务不可用，但是数据还是一致的。
一旦你把数据和代码逻辑，开始部署在不同的节点上，这时候就存在分区。如Node A 不能和Node B通信来，这种分区问题经常出现。
用图来证明：
在一个网络环境下，有两个节点，N1和N2，共享相同的数据V，在买书这个例子中，这个数据里面存储的就是有多少本书，假设初始值是V0，在N1上运行一个买卖算法，A，假设这个算法没有bug，非常正确，可心来，N2也是类似的，叫做B，A写了一个新值到V中，然后B从V中读取。
正常流程是这样，A写完之后，N1和N2通过一个消息（非具体的消息），将这个值同步给N2。然后B也就能读到了。
1.A写了一个值V1 2.从N1发了个消息M到N2。 3.B也能从V中读到V1了
但是，现实没有这么美好
网络发生了分区，从N1到N2的消息没有投递成功。这样，到第三步的时候，N2读到了V0这个错误的值。
如果M是一个异步消息，那么N1都没办法知道N2是不是收到了。即使有办法保证M这个消息一定发出去了。那么N1也没办法知道，这个消息是不是被投递了，也不知道N2处理的时候，有没有问题。那么，如果我们把M改成同步消息呢。也不行，因为这意味着将A写值到N1，和从N1到N2更新事件是一个原子操作。
CAP告诉我们，如果我们想要A和B高度可用（低延迟），我们就要N1和N2保持分区容错，比如出现消息丢失，消息未投递，硬件故障，或者处理失败。这种情况下，就会出现有时候一些节点任务V是V0，另一些节点认为是V1.
如果有一个事务，叫做a1，a1可能是一个写操作，a2是一个读操作，在本地系统中，通过数据库或者自己加锁，加隔离是很简单的。可以强制a1写完之后，a2才发生，但是在分布式环境中，一旦加了这些东西，就影响额分区容错和可用性。
处理CAP  CA 不要 P，不要分区容错  不保证分区容错，那么你可以部署在一个台机器上，但是容量受限。并且还是会存在网络问题。分布式环境下，网络分区是必然的。除非你就不想做分布式。
在分布式的环境下，网络无法做到100%可靠，有可能出现故障，因此分区是一个必须的选项，如果选择了CA而放弃了P，若发生分区现象，为了保证C，系统需要禁止写入，此时就与A发生冲突，如果是为了保证A，则会出现正常的分区可以写入数据，有故障的分区不能写入数据，则与C就冲突了。因此分布式系统理论上不可能选择CA架构，而必须选择CP或AP架构。
从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。
所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。
CP 不要 A 不要可用性  当你想要分区容错的时候，并且可以容忍长时间的停机或者无影响。就可以舍弃可用性。
一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。
设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，
常用的Zookeeper也是在CAP三者之中选择优先保证CP的。ZooKeeper是个CP 的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分区具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。
ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成CP而不是AP特性的了。从实际情况来分析，在使用 Zookeeper 获取服务列表时，如果 ZooKeeper 正在选举或者 ZooKeeper 集群中半数以上的机器不可用，那么将无法获取数据。所以说，ZooKeeper 不能保证服务可用性。
Eureka 则是一个AP系统，一部分节点挂掉不会影响到正常节点的工作，不会出现类似 ZK 的选举 Leader 的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点。 只要有一台 Eureka 存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。
SofaRegistry 也是一个AP系统。
AP 不要 C 不要一致性  要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。
这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。
举个例子，你在12306买票的时候肯定遇到过这种场景，你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。
但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。也就是说，最终不会出现，2个人买到了同样的票。
对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。
怎么选择呢 既要又要。那怎么办？
虽然三个不能保证，但我们能不能在一致性上作出一些妥协，不追求时时刻刻的强一致性，转而追求最终一致性，所以引入 BASE 理论。 在分布式事务中，BASE 最重要是为 CAP 提出了最终一致性的解决方案，BASE 强调牺牲高一致性，从而获取可用性，数据允许在一段时间内不一致，只要保证最终一致性就可以了，实现最终一致性。
弱一致性：系统不能保证后续访问返回更新的值。需要在一些条件满足之后，更新的值才能返回。从更新操作开始，到系统保证任何观察者总是看到更新的值的这期间被称为不一致窗口。
最终一致性：这是弱一致性的特殊形式;存储系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。
BASE 模型 BASE 模型是传统 ACID 模型的反面，不同于 ACID，BASE 强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。 BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性：Basically Available 基本可用。 支持分区失败(e.g. sharding碎片划分数据库)Soft state 软状态，状态可以有一段时间不同步，异步。 Eventually consistent 最终一致，最终数据是一致的就可以了，而不是时时一致。
参考 http://www.julianbrowne.com/article/brewers-cap-theorem
https://www.cnblogs.com/13yan/p/9243669.html"><meta property="og:type" content="article"><meta property="og:url" content="http://blog.leaver.me/2020/01/06/%E7%90%86%E8%A7%A3-cap-%E7%90%86%E8%AE%BA/"><meta property="article:published_time" content="2020-01-06T10:00:08+08:00"><meta property="article:modified_time" content="2020-01-06T10:00:08+08:00"><meta itemprop=name content="理解 CAP 理论"><meta itemprop=description content="背景 CAP理论 实际上听起来非常简单，但是有时候，遇到一些具体的问题的时候， 还是不能很清晰的分辨出来，到底是CP还是AP，以及一些其他的问题。因此，专门作为&#34;生产者&#34;来学习下，加深理解。
首先，在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer&rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
1.一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 2.可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 3.分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）
根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。
这个定理起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在2000年的分布式计算原理研讨会（PODC）上提出的一个猜想。 在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。
举例 假设你明天就要放长假了，你想买一本战争与和平的书籍，你最喜欢的在线商城里面只有一本了。
一致性： Consistency，在Gilbert and Lynch 的论文里，他们也用 “Atomic” 原子性来代替一致性这个单词。 在买书的这个例子里，你要么就是把书放到了购物车，要么就是放失败了，要么付款，要么没付款，不可能说放了一半，或者说买了一半。只有一本书，如果两个客户都准备买，缺乏一致性的话，如果两个人都完成了下单，可能会出问题。比如两个人都下了单，当然，在这个例子中，并不严重。
我们也可以用数据库来解决这个问题，数据库里有个字段减去个1，然后当及其他客户也要付款的时候，我们提示他没了。
数据库看很好用。因为具有ACID的能力。既有一致性，又有原子性，中间状态对第二个客户端是不可见的。是隔离的。因为第二个客户端再下单的时候，另一个用户在事务中的话，就锁住了数据库那条记录。
可用性： 可用性就是说，当你需要的时候，大部分情况下，服务都是可以为你服务的。以买书为例，在用户A开启事务的时候，有那么几毫秒是锁表的，这个阶段，服务可以认为对其他用户是不可用的。并不是说要时时刻刻可用，一般会有个可用率的指标。如果记不住，可以通过这里来计算： https://uptime.is/99.99999
分区容错： 如果你就一个数据库，一个服务端，那一般也都是原子的，如果挂了，服务不可用，但是数据还是一致的。
一旦你把数据和代码逻辑，开始部署在不同的节点上，这时候就存在分区。如Node A 不能和Node B通信来，这种分区问题经常出现。
用图来证明：
在一个网络环境下，有两个节点，N1和N2，共享相同的数据V，在买书这个例子中，这个数据里面存储的就是有多少本书，假设初始值是V0，在N1上运行一个买卖算法，A，假设这个算法没有bug，非常正确，可心来，N2也是类似的，叫做B，A写了一个新值到V中，然后B从V中读取。
正常流程是这样，A写完之后，N1和N2通过一个消息（非具体的消息），将这个值同步给N2。然后B也就能读到了。
1.A写了一个值V1 2.从N1发了个消息M到N2。 3.B也能从V中读到V1了
但是，现实没有这么美好
网络发生了分区，从N1到N2的消息没有投递成功。这样，到第三步的时候，N2读到了V0这个错误的值。
如果M是一个异步消息，那么N1都没办法知道N2是不是收到了。即使有办法保证M这个消息一定发出去了。那么N1也没办法知道，这个消息是不是被投递了，也不知道N2处理的时候，有没有问题。那么，如果我们把M改成同步消息呢。也不行，因为这意味着将A写值到N1，和从N1到N2更新事件是一个原子操作。
CAP告诉我们，如果我们想要A和B高度可用（低延迟），我们就要N1和N2保持分区容错，比如出现消息丢失，消息未投递，硬件故障，或者处理失败。这种情况下，就会出现有时候一些节点任务V是V0，另一些节点认为是V1.
如果有一个事务，叫做a1，a1可能是一个写操作，a2是一个读操作，在本地系统中，通过数据库或者自己加锁，加隔离是很简单的。可以强制a1写完之后，a2才发生，但是在分布式环境中，一旦加了这些东西，就影响额分区容错和可用性。
处理CAP  CA 不要 P，不要分区容错  不保证分区容错，那么你可以部署在一个台机器上，但是容量受限。并且还是会存在网络问题。分布式环境下，网络分区是必然的。除非你就不想做分布式。
在分布式的环境下，网络无法做到100%可靠，有可能出现故障，因此分区是一个必须的选项，如果选择了CA而放弃了P，若发生分区现象，为了保证C，系统需要禁止写入，此时就与A发生冲突，如果是为了保证A，则会出现正常的分区可以写入数据，有故障的分区不能写入数据，则与C就冲突了。因此分布式系统理论上不可能选择CA架构，而必须选择CP或AP架构。
从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。
所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。
CP 不要 A 不要可用性  当你想要分区容错的时候，并且可以容忍长时间的停机或者无影响。就可以舍弃可用性。
一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。
设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，
常用的Zookeeper也是在CAP三者之中选择优先保证CP的。ZooKeeper是个CP 的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分区具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。
ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成CP而不是AP特性的了。从实际情况来分析，在使用 Zookeeper 获取服务列表时，如果 ZooKeeper 正在选举或者 ZooKeeper 集群中半数以上的机器不可用，那么将无法获取数据。所以说，ZooKeeper 不能保证服务可用性。
Eureka 则是一个AP系统，一部分节点挂掉不会影响到正常节点的工作，不会出现类似 ZK 的选举 Leader 的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点。 只要有一台 Eureka 存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。
SofaRegistry 也是一个AP系统。
AP 不要 C 不要一致性  要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。
这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。
举个例子，你在12306买票的时候肯定遇到过这种场景，你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。
但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。也就是说，最终不会出现，2个人买到了同样的票。
对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。
怎么选择呢 既要又要。那怎么办？
虽然三个不能保证，但我们能不能在一致性上作出一些妥协，不追求时时刻刻的强一致性，转而追求最终一致性，所以引入 BASE 理论。 在分布式事务中，BASE 最重要是为 CAP 提出了最终一致性的解决方案，BASE 强调牺牲高一致性，从而获取可用性，数据允许在一段时间内不一致，只要保证最终一致性就可以了，实现最终一致性。
弱一致性：系统不能保证后续访问返回更新的值。需要在一些条件满足之后，更新的值才能返回。从更新操作开始，到系统保证任何观察者总是看到更新的值的这期间被称为不一致窗口。
最终一致性：这是弱一致性的特殊形式;存储系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。
BASE 模型 BASE 模型是传统 ACID 模型的反面，不同于 ACID，BASE 强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。 BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性：Basically Available 基本可用。 支持分区失败(e.g. sharding碎片划分数据库)Soft state 软状态，状态可以有一段时间不同步，异步。 Eventually consistent 最终一致，最终数据是一致的就可以了，而不是时时一致。
参考 http://www.julianbrowne.com/article/brewers-cap-theorem
https://www.cnblogs.com/13yan/p/9243669.html"><meta itemprop=datePublished content="2020-01-06T10:00:08+08:00"><meta itemprop=dateModified content="2020-01-06T10:00:08+08:00"><meta itemprop=wordCount content="136"><meta itemprop=keywords content="分布式,理论,"><meta name=twitter:card content="summary"><meta name=twitter:title content="理解 CAP 理论"><meta name=twitter:description content="背景 CAP理论 实际上听起来非常简单，但是有时候，遇到一些具体的问题的时候， 还是不能很清晰的分辨出来，到底是CP还是AP，以及一些其他的问题。因此，专门作为&#34;生产者&#34;来学习下，加深理解。
首先，在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer&rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
1.一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 2.可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 3.分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）
根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。
这个定理起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在2000年的分布式计算原理研讨会（PODC）上提出的一个猜想。 在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。
举例 假设你明天就要放长假了，你想买一本战争与和平的书籍，你最喜欢的在线商城里面只有一本了。
一致性： Consistency，在Gilbert and Lynch 的论文里，他们也用 “Atomic” 原子性来代替一致性这个单词。 在买书的这个例子里，你要么就是把书放到了购物车，要么就是放失败了，要么付款，要么没付款，不可能说放了一半，或者说买了一半。只有一本书，如果两个客户都准备买，缺乏一致性的话，如果两个人都完成了下单，可能会出问题。比如两个人都下了单，当然，在这个例子中，并不严重。
我们也可以用数据库来解决这个问题，数据库里有个字段减去个1，然后当及其他客户也要付款的时候，我们提示他没了。
数据库看很好用。因为具有ACID的能力。既有一致性，又有原子性，中间状态对第二个客户端是不可见的。是隔离的。因为第二个客户端再下单的时候，另一个用户在事务中的话，就锁住了数据库那条记录。
可用性： 可用性就是说，当你需要的时候，大部分情况下，服务都是可以为你服务的。以买书为例，在用户A开启事务的时候，有那么几毫秒是锁表的，这个阶段，服务可以认为对其他用户是不可用的。并不是说要时时刻刻可用，一般会有个可用率的指标。如果记不住，可以通过这里来计算： https://uptime.is/99.99999
分区容错： 如果你就一个数据库，一个服务端，那一般也都是原子的，如果挂了，服务不可用，但是数据还是一致的。
一旦你把数据和代码逻辑，开始部署在不同的节点上，这时候就存在分区。如Node A 不能和Node B通信来，这种分区问题经常出现。
用图来证明：
在一个网络环境下，有两个节点，N1和N2，共享相同的数据V，在买书这个例子中，这个数据里面存储的就是有多少本书，假设初始值是V0，在N1上运行一个买卖算法，A，假设这个算法没有bug，非常正确，可心来，N2也是类似的，叫做B，A写了一个新值到V中，然后B从V中读取。
正常流程是这样，A写完之后，N1和N2通过一个消息（非具体的消息），将这个值同步给N2。然后B也就能读到了。
1.A写了一个值V1 2.从N1发了个消息M到N2。 3.B也能从V中读到V1了
但是，现实没有这么美好
网络发生了分区，从N1到N2的消息没有投递成功。这样，到第三步的时候，N2读到了V0这个错误的值。
如果M是一个异步消息，那么N1都没办法知道N2是不是收到了。即使有办法保证M这个消息一定发出去了。那么N1也没办法知道，这个消息是不是被投递了，也不知道N2处理的时候，有没有问题。那么，如果我们把M改成同步消息呢。也不行，因为这意味着将A写值到N1，和从N1到N2更新事件是一个原子操作。
CAP告诉我们，如果我们想要A和B高度可用（低延迟），我们就要N1和N2保持分区容错，比如出现消息丢失，消息未投递，硬件故障，或者处理失败。这种情况下，就会出现有时候一些节点任务V是V0，另一些节点认为是V1.
如果有一个事务，叫做a1，a1可能是一个写操作，a2是一个读操作，在本地系统中，通过数据库或者自己加锁，加隔离是很简单的。可以强制a1写完之后，a2才发生，但是在分布式环境中，一旦加了这些东西，就影响额分区容错和可用性。
处理CAP  CA 不要 P，不要分区容错  不保证分区容错，那么你可以部署在一个台机器上，但是容量受限。并且还是会存在网络问题。分布式环境下，网络分区是必然的。除非你就不想做分布式。
在分布式的环境下，网络无法做到100%可靠，有可能出现故障，因此分区是一个必须的选项，如果选择了CA而放弃了P，若发生分区现象，为了保证C，系统需要禁止写入，此时就与A发生冲突，如果是为了保证A，则会出现正常的分区可以写入数据，有故障的分区不能写入数据，则与C就冲突了。因此分布式系统理论上不可能选择CA架构，而必须选择CP或AP架构。
从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。
所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。
CP 不要 A 不要可用性  当你想要分区容错的时候，并且可以容忍长时间的停机或者无影响。就可以舍弃可用性。
一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。
设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，
常用的Zookeeper也是在CAP三者之中选择优先保证CP的。ZooKeeper是个CP 的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分区具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。
ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成CP而不是AP特性的了。从实际情况来分析，在使用 Zookeeper 获取服务列表时，如果 ZooKeeper 正在选举或者 ZooKeeper 集群中半数以上的机器不可用，那么将无法获取数据。所以说，ZooKeeper 不能保证服务可用性。
Eureka 则是一个AP系统，一部分节点挂掉不会影响到正常节点的工作，不会出现类似 ZK 的选举 Leader 的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点。 只要有一台 Eureka 存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。
SofaRegistry 也是一个AP系统。
AP 不要 C 不要一致性  要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。
这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。
举个例子，你在12306买票的时候肯定遇到过这种场景，你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。
但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。也就是说，最终不会出现，2个人买到了同样的票。
对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。
怎么选择呢 既要又要。那怎么办？
虽然三个不能保证，但我们能不能在一致性上作出一些妥协，不追求时时刻刻的强一致性，转而追求最终一致性，所以引入 BASE 理论。 在分布式事务中，BASE 最重要是为 CAP 提出了最终一致性的解决方案，BASE 强调牺牲高一致性，从而获取可用性，数据允许在一段时间内不一致，只要保证最终一致性就可以了，实现最终一致性。
弱一致性：系统不能保证后续访问返回更新的值。需要在一些条件满足之后，更新的值才能返回。从更新操作开始，到系统保证任何观察者总是看到更新的值的这期间被称为不一致窗口。
最终一致性：这是弱一致性的特殊形式;存储系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。
BASE 模型 BASE 模型是传统 ACID 模型的反面，不同于 ACID，BASE 强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。 BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性：Basically Available 基本可用。 支持分区失败(e.g. sharding碎片划分数据库)Soft state 软状态，状态可以有一段时间不同步，异步。 Eventually consistent 最终一致，最终数据是一致的就可以了，而不是时时一致。
参考 http://www.julianbrowne.com/article/brewers-cap-theorem
https://www.cnblogs.com/13yan/p/9243669.html"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-30961201-3','auto');ga('send','pageview');}</script></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>bystander's blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=http://blog.leaver.me/>主页</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://blog.leaver.me/post/>归档</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://blog.leaver.me/tags/>标签</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://blog.leaver.me/categories/>目录</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://blog.leaver.me/about/>关于我</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>bystander's blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=http://blog.leaver.me/>主页</a></li><li class=menu-item><a class=menu-item-link href=http://blog.leaver.me/post/>归档</a></li><li class=menu-item><a class=menu-item-link href=http://blog.leaver.me/tags/>标签</a></li><li class=menu-item><a class=menu-item-link href=http://blog.leaver.me/categories/>目录</a></li><li class=menu-item><a class=menu-item-link href=http://blog.leaver.me/about/>关于我</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight wallpaper"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>理解 CAP 理论</h1><div class=post-meta><div class=post-meta-author>by
bystander</div><div class=post-meta-time><time datetime=2020-01-06>2020-01-06</time></div><div class=post-meta__right><span class=post-meta-more>约 136 字 -
预计阅读 1 分钟</span><div class=post-meta-category><a href=http://blog.leaver.me/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>学习笔记</a></div></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#一致性>一致性：</a></li><li><a href=#可用性>可用性：</a></li><li><a href=#分区容错>分区容错：</a></li></ul><ul><li><a href=#base-模型>BASE 模型</a></li></ul></nav></div></div><div class=post-content><h1 id=背景>背景</h1><p>CAP理论 实际上听起来非常简单，但是有时候，遇到一些具体的问题的时候， 还是不能很清晰的分辨出来，到底是CP还是AP，以及一些其他的问题。因此，专门作为"生产者"来学习下，加深理解。</p><p>首先，在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer&rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：</p><p>1.一致性（Consistency） （等同于所有节点访问同一份最新的数据副本）
2.可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据）
3.分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）</p><p>根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。</p><p>这个定理起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在2000年的分布式计算原理研讨会（PODC）上提出的一个猜想。 在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。</p><h1 id=举例>举例</h1><p>假设你明天就要放长假了，你想买一本战争与和平的书籍，你最喜欢的在线商城里面只有一本了。</p><h2 id=一致性>一致性：</h2><p>Consistency，在Gilbert and Lynch 的论文里，他们也用 “Atomic” 原子性来代替一致性这个单词。
在买书的这个例子里，你要么就是把书放到了购物车，要么就是放失败了，要么付款，要么没付款，不可能说放了一半，或者说买了一半。只有一本书，如果两个客户都准备买，缺乏一致性的话，如果两个人都完成了下单，可能会出问题。比如两个人都下了单，当然，在这个例子中，并不严重。</p><p>我们也可以用数据库来解决这个问题，数据库里有个字段减去个1，然后当及其他客户也要付款的时候，我们提示他没了。</p><p>数据库看很好用。因为具有ACID的能力。既有一致性，又有原子性，中间状态对第二个客户端是不可见的。是隔离的。因为第二个客户端再下单的时候，另一个用户在事务中的话，就锁住了数据库那条记录。</p><h2 id=可用性>可用性：</h2><p>可用性就是说，当你需要的时候，大部分情况下，服务都是可以为你服务的。以买书为例，在用户A开启事务的时候，有那么几毫秒是锁表的，这个阶段，服务可以认为对其他用户是不可用的。并不是说要时时刻刻可用，一般会有个可用率的指标。如果记不住，可以通过这里来计算：
<a href=https://uptime.is/99.99999>https://uptime.is/99.99999</a></p><h2 id=分区容错>分区容错：</h2><p>如果你就一个数据库，一个服务端，那一般也都是原子的，如果挂了，服务不可用，但是数据还是一致的。</p><p>一旦你把数据和代码逻辑，开始部署在不同的节点上，这时候就存在分区。如Node A 不能和Node B通信来，这种分区问题经常出现。</p><p>用图来证明：</p><p><img src=/images/2020-01-06-10-43-03.png alt=基础问题></p><p>在一个网络环境下，有两个节点，N1和N2，共享相同的数据V，在买书这个例子中，这个数据里面存储的就是有多少本书，假设初始值是V0，在N1上运行一个买卖算法，A，假设这个算法没有bug，非常正确，可心来，N2也是类似的，叫做B，A写了一个新值到V中，然后B从V中读取。</p><p><img src=/images/2020-01-06-10-45-02.png alt=正常流程></p><p>正常流程是这样，A写完之后，N1和N2通过一个消息（非具体的消息），将这个值同步给N2。然后B也就能读到了。</p><p>1.A写了一个值V1
2.从N1发了个消息M到N2。
3.B也能从V中读到V1了</p><p>但是，现实没有这么美好</p><p><img src=/images/2020-01-06-10-47-10.png alt=分区></p><p>网络发生了分区，从N1到N2的消息没有投递成功。这样，到第三步的时候，N2读到了V0这个错误的值。</p><p>如果M是一个异步消息，那么N1都没办法知道N2是不是收到了。即使有办法保证M这个消息一定发出去了。那么N1也没办法知道，这个消息是不是被投递了，也不知道N2处理的时候，有没有问题。那么，如果我们把M改成同步消息呢。也不行，因为这意味着将A写值到N1，和从N1到N2更新事件是一个原子操作。</p><p>CAP告诉我们，如果我们想要A和B高度可用（低延迟），我们就要N1和N2保持分区容错，比如出现消息丢失，消息未投递，硬件故障，或者处理失败。这种情况下，就会出现有时候一些节点任务V是V0，另一些节点认为是V1.</p><p><img src=/images/2020-01-06-11-05-49.png alt=事务></p><p>如果有一个事务，叫做a1，a1可能是一个写操作，a2是一个读操作，在本地系统中，通过数据库或者自己加锁，加隔离是很简单的。可以强制a1写完之后，a2才发生，但是在分布式环境中，一旦加了这些东西，就影响额分区容错和可用性。</p><h1 id=处理cap>处理CAP</h1><ol><li>CA 不要 P，不要分区容错</li></ol><p>不保证分区容错，那么你可以部署在一个台机器上，但是容量受限。并且还是会存在网络问题。分布式环境下，网络分区是必然的。除非你就不想做分布式。</p><p>在分布式的环境下，网络无法做到100%可靠，有可能出现故障，因此分区是一个必须的选项，如果选择了CA而放弃了P，若发生分区现象，为了保证C，系统需要禁止写入，此时就与A发生冲突，如果是为了保证A，则会出现正常的分区可以写入数据，有故障的分区不能写入数据，则与C就冲突了。因此分布式系统理论上不可能选择CA架构，而必须选择CP或AP架构。</p><p>从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。</p><p>所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。</p><ol start=2><li>CP 不要 A 不要可用性</li></ol><p>当你想要分区容错的时候，并且可以容忍长时间的停机或者无影响。就可以舍弃可用性。</p><p>一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。</p><p>设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，</p><p>常用的Zookeeper也是在CAP三者之中选择优先保证CP的。ZooKeeper是个CP 的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分区具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。</p><p>ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成CP而不是AP特性的了。从实际情况来分析，在使用 Zookeeper 获取服务列表时，如果 ZooKeeper 正在选举或者 ZooKeeper 集群中半数以上的机器不可用，那么将无法获取数据。所以说，ZooKeeper 不能保证服务可用性。</p><p>Eureka 则是一个AP系统，一部分节点挂掉不会影响到正常节点的工作，不会出现类似 ZK 的选举 Leader 的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点。
只要有一台 Eureka 存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。</p><p>SofaRegistry 也是一个AP系统。</p><ol start=3><li>AP 不要 C 不要一致性</li></ol><p>要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。</p><p>这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p><p>举个例子，你在12306买票的时候肯定遇到过这种场景，你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。</p><p>但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。也就是说，最终不会出现，2个人买到了同样的票。</p><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p><h1 id=怎么选择呢>怎么选择呢</h1><p>既要又要。那怎么办？</p><p>虽然三个不能保证，但我们能不能在一致性上作出一些妥协，不追求时时刻刻的强一致性，转而追求最终一致性，所以引入 BASE 理论。
在分布式事务中，BASE 最重要是为 CAP 提出了最终一致性的解决方案，BASE 强调牺牲高一致性，从而获取可用性，数据允许在一段时间内不一致，只要保证最终一致性就可以了，实现最终一致性。</p><p>弱一致性：系统不能保证后续访问返回更新的值。需要在一些条件满足之后，更新的值才能返回。从更新操作开始，到系统保证任何观察者总是看到更新的值的这期间被称为不一致窗口。</p><p>最终一致性：这是弱一致性的特殊形式;存储系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。</p><h2 id=base-模型>BASE 模型</h2><p>BASE 模型是传统 ACID 模型的反面，不同于 ACID，BASE 强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。
BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性：Basically Available 基本可用。
支持分区失败(e.g. sharding碎片划分数据库)Soft state 软状态，状态可以有一段时间不同步，异步。
Eventually consistent 最终一致，最终数据是一致的就可以了，而不是时时一致。</p><h1 id=参考>参考</h1><p><a href=http://www.julianbrowne.com/article/brewers-cap-theorem>http://www.julianbrowne.com/article/brewers-cap-theorem</a></p><p><a href=https://www.cnblogs.com/13yan/p/9243669.html>https://www.cnblogs.com/13yan/p/9243669.html</a></p></div><footer class=post-footer><div class=post-tags><a href=http://blog.leaver.me/tags/%E5%88%86%E5%B8%83%E5%BC%8F/>分布式</a>
<a href=http://blog.leaver.me/tags/%E7%90%86%E8%AE%BA/>理论</a></div><nav class=post-nav><a class=prev href=/2020/01/08/2019-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">2019 年终总结</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/2019/12/30/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3raft%E5%8D%8F%E8%AE%AE/><span class="next-text nav-default">深入理解Raft协议</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article><div class=disqus-comment><div class=disqus-button id=load_disqus onclick=load_disqus()>显示 Disqus 评论</div><div id=disqus_thread></div><script type=text/javascript>var disqus_config=function(){this.page.url="http://blog.leaver.me/2020/01/06/%E7%90%86%E8%A7%A3-cap-%E7%90%86%E8%AE%BA/";};function load_disqus(){if(window.location.hostname==='localhost')return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='leaverme';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);$('#load_disqus').remove();};</script><noscript>Please enable JavaScript to view the
<a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=https://github.com/leizhiyuan rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=http://blog.leaver.me/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2012 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>bystander</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.fe83e11b4fbc9193d67e2c9db78bad21f8dc59fca0cacd8c1c3bb071bb16a852.js integrity="sha256-/oPhG0+8kZPWfiydt4utIfjcWfygys2MHDuwcbsWqFI=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>